{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本预处理\n",
    "- 读入文本\n",
    "- 分词\n",
    "- 建立词典\n",
    "- 将文本从词的序列转换为索引的序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入文本\n",
    "以英文小说Time Machine(..\\data\\04time_machine.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Project\\Anaconda\\dive2DL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, os.path.join(BASE_DIR))\n",
    "print(BASE_DIR)\n",
    "\n",
    "import collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_time_machine():\n",
    "    with open(os.path.join(BASE_DIR, \"data\", \"04time_machine.txt\"), 'r', encoding='utf-8') as f:\n",
    "        lines = [re.sub('[^a-z]+', ' ', line.strip().lower()) for line in f]\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences 3583\n"
     ]
    }
   ],
   "source": [
    "lines = read_time_machine()\n",
    "print('# sentences %d' % len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[''],\n",
       " ['the',\n",
       "  'project',\n",
       "  'gutenberg',\n",
       "  'ebook',\n",
       "  'of',\n",
       "  'the',\n",
       "  'time',\n",
       "  'machine',\n",
       "  'by',\n",
       "  'h',\n",
       "  'g',\n",
       "  'wells']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(sentences, token='word'):\n",
    "    if token == 'word':\n",
    "        return [sentence.split(' ') for sentence in sentences ]\n",
    "    if token == 'char':\n",
    "        return [list(sentence) for sentence in sentences]\n",
    "    else:\n",
    "        print(\"ERROR: unkown token type \"+token)\n",
    "\n",
    "tokens = tokenize(lines)\n",
    "tokens[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n",
    "        counter = count_corpus(tokens)\n",
    "        self.token_freqs = list(counter.items())\n",
    "        self.idx_to_token = []\n",
    "        if use_special_tokens:\n",
    "            # padding, begin, end, unknown\n",
    "            self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n",
    "            self.idx_to_token += ['', '', '', '']\n",
    "        else: \n",
    "            self.unk = 0\n",
    "            self.idx_to_token += ['']\n",
    "        self.idx_to_token += [token for token, freq in self.token_freqs \n",
    "                             if freq >= min_freq and token not in self.idx_to_token]\n",
    "        self.token_to_idx = dict()\n",
    "        for idx, token in enumerate(self.idx_to_token):\n",
    "            self.token_to_idx[token] = idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "def count_corpus(sentences):\n",
    "    tokens = [tk for st in sentences for tk in st]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 0), ('the', 1), ('project', 2), ('gutenberg', 3), ('ebook', 4), ('of', 5), ('time', 6), ('machine', 7), ('by', 8), ('h', 9), ('g', 10), ('wells', 11), ('this', 12), ('is', 13), ('for', 14), ('use', 15), ('anyone', 16), ('anywhere', 17), ('at', 18), ('no', 19)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(tokens)\n",
    "print(list(vocab.token_to_idx.items())[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将词转换为索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['']\n",
      "indices: [0]\n",
      "words: ['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "indices: [1, 2, 3, 4, 5, 1, 6, 7, 8, 9, 10, 11]\n",
      "words: ['']\n",
      "indices: [0]\n",
      "words: ['this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with']\n",
      "indices: [12, 4, 13, 14, 1, 15, 5, 16, 17, 18, 19, 20, 21, 22]\n",
      "words: ['almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or']\n",
      "indices: [23, 19, 24, 25, 26, 27, 28, 29, 30, 29, 31, 32]\n",
      "words: ['re', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included']\n",
      "indices: [33, 15, 29, 34, 1, 35, 5, 1, 2, 3, 36, 37]\n",
      "words: ['with', 'this', 'ebook', 'or', 'online', 'at', 'www', 'gutenberg', 'net']\n",
      "indices: [22, 12, 4, 32, 38, 18, 39, 3, 40]\n",
      "words: ['']\n",
      "indices: [0]\n",
      "words: ['']\n",
      "indices: [0]\n",
      "words: ['title', 'the', 'time', 'machine']\n",
      "indices: [41, 1, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print('words:', tokens[i])\n",
    "    print('indices:', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
