{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力和seq2seq模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力机制框架\n",
    "Attention 是一种通用的带权池化方法，对于一个query来说，attention layer 会与每一个key计算注意力分数并进行权重的归一化，输出的向量 o 则是value的加权求和，而每个key计算的权重与value一一对应。\n",
    "\n",
    "为了计算输出，我们首先假设有一个函数 α  用于计算query和key的相似性，然后可以计算所有的 attention scores  a1,…,an  by $a_i = a(q, k_i)$\n",
    "使用softmax函数获得注意力权重：\n",
    "$$b_1, \\dots, b_n = softmax(a_1, \\dots, a_n)$$\n",
    "最终的输出就是value的加权求和：\n",
    "$$o = \\sum\\limits_{i=1}^nb_iv_i$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
