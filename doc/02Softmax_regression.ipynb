{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n",
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "# import needed package\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "\n",
    "import sys\n",
    "import utils\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取和读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Project\\Anaconda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.insert(0, os.path.join(BASE_DIR))\n",
    "print(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to D:\\Project\\Anaconda\\dataset\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████████████████████████████████▏ | 25698304/26421880 [00:20<00:00, 1916280.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:\\Project\\Anaconda\\dataset\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to D:\\Project\\Anaconda\\dataset\\FashionMNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to D:\\Project\\Anaconda\\dataset\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/29515 [00:02<?, ?it/s]\u001b[A\n",
      "32768it [00:02, 11024.54it/s]                                                                                          \u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:\\Project\\Anaconda\\dataset\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to D:\\Project\\Anaconda\\dataset\\FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to D:\\Project\\Anaconda\\dataset\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                      | 0/4422102 [00:02<?, ?it/s]\u001b[A\n",
      "  0%|▎                                                                      | 16384/4422102 [00:02<01:10, 62317.14it/s]\u001b[A\n",
      "  1%|▍                                                                      | 24576/4422102 [00:02<01:15, 58176.38it/s]\u001b[A\n",
      "  1%|▌                                                                      | 32768/4422102 [00:02<01:21, 53807.60it/s]\u001b[A\n",
      "  2%|█▌                                                                     | 98304/4422102 [00:03<00:58, 74058.59it/s]\u001b[A\n",
      "  3%|█▉                                                                    | 122880/4422102 [00:03<01:13, 58775.04it/s]\u001b[A\n",
      "  5%|███▊                                                                  | 237568/4422102 [00:03<00:51, 80628.69it/s]\u001b[A\n",
      "  9%|██████▌                                                               | 417792/4422102 [00:04<00:40, 99739.46it/s]\u001b[A\n",
      " 15%|██████████▌                                                          | 679936/4422102 [00:04<00:27, 137890.28it/s]\u001b[A\n",
      " 17%|███████████▋                                                         | 745472/4422102 [00:05<00:23, 159431.16it/s]\u001b[A\n",
      " 19%|█████████████▏                                                       | 843776/4422102 [00:05<00:19, 188331.65it/s]\u001b[A\n",
      " 26%|█████████████████▍                                                  | 1130496/4422102 [00:05<00:12, 260110.57it/s]\u001b[A\n",
      " 28%|███████████████████                                                 | 1236992/4422102 [00:06<00:13, 243638.83it/s]\u001b[A\n",
      " 33%|██████████████████████▍                                             | 1458176/4422102 [00:06<00:08, 330423.16it/s]\u001b[A\n",
      " 36%|████████████████████████▏                                           | 1572864/4422102 [00:06<00:09, 296473.39it/s]\u001b[A\n",
      " 40%|███████████████████████████▍                                        | 1785856/4422102 [00:06<00:07, 370517.60it/s]\u001b[A\n",
      " 42%|████████████████████████████▊                                       | 1875968/4422102 [00:07<00:07, 338018.32it/s]\u001b[A\n",
      " 45%|██████████████████████████████▊                                     | 2007040/4422102 [00:07<00:05, 419475.33it/s]\u001b[A\n",
      " 47%|████████████████████████████████                                    | 2088960/4422102 [00:07<00:04, 489503.92it/s]\u001b[A\n",
      " 49%|█████████████████████████████████▍                                  | 2170880/4422102 [00:07<00:07, 319886.34it/s]\u001b[A\n",
      " 54%|████████████████████████████████████▉                               | 2400256/4422102 [00:08<00:04, 419447.23it/s]\u001b[A\n",
      " 56%|██████████████████████████████████████▎                             | 2490368/4422102 [00:08<00:04, 430268.92it/s]\u001b[A\n",
      " 58%|███████████████████████████████████████▋                            | 2580480/4422102 [00:08<00:06, 304012.50it/s]\u001b[A\n",
      " 61%|█████████████████████████████████████████▌                          | 2703360/4422102 [00:09<00:05, 330034.09it/s]\u001b[A\n",
      " 64%|███████████████████████████████████████████▏                        | 2809856/4422102 [00:09<00:05, 287870.75it/s]\u001b[A\n",
      " 69%|██████████████████████████████████████████████▋                     | 3039232/4422102 [00:09<00:03, 373916.62it/s]\u001b[A\n",
      " 74%|██████████████████████████████████████████████████                  | 3252224/4422102 [00:10<00:02, 408330.73it/s]\u001b[A\n",
      " 77%|████████████████████████████████████████████████████                | 3383296/4422102 [00:10<00:02, 449670.99it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████▍             | 3538944/4422102 [00:10<00:01, 466686.79it/s]\u001b[A\n",
      " 82%|███████████████████████████████████████████████████████▋            | 3620864/4422102 [00:10<00:01, 491137.25it/s]\u001b[A\n",
      " 85%|█████████████████████████████████████████████████████████▌          | 3743744/4422102 [00:11<00:01, 520943.86it/s]\u001b[A\n",
      " 89%|████████████████████████████████████████████████████████████▊       | 3956736/4422102 [00:11<00:00, 658381.09it/s]\u001b[A\n",
      " 92%|██████████████████████████████████████████████████████████████▎     | 4055040/4422102 [00:11<00:00, 679752.09it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████████████████████████████    | 4169728/4422102 [00:11<00:00, 625827.01it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████████████████████████████▍  | 4251648/4422102 [00:11<00:00, 653917.64it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████████████████████████████▋ | 4333568/4422102 [00:11<00:00, 685294.58it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████▉| 4415488/4422102 [00:11<00:00, 680662.48it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:\\Project\\Anaconda\\dataset\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to D:\\Project\\Anaconda\\dataset\\FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to D:\\Project\\Anaconda\\dataset\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "8192it [00:03, 2108.81it/s]                                                                                            \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:\\Project\\Anaconda\\dataset\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to D:\\Project\\Anaconda\\dataset\\FashionMNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "26427392it [00:40, 1916280.51it/s]                                                                                     \n",
      "4423680it [00:26, 680662.48it/s]                                                                                       \u001b[A"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = utils.load_data_fashion_mnist(batch_size, root=os.path.join(BASE_DIR,'dataset'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "W = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_outputs)), dtype=torch.float)\n",
    "b = torch.zeros(num_outputs, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.requires_grad_(requires_grad=True)\n",
    "b.requires_grad_(requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax\n",
    "$$\\hat y_j = \\frac{exp(o_j)}{\\sum\\limits_{i=1}^3 exp(o_i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(dim=1, keepdim=True)\n",
    "    return X_exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((2, 5))\n",
    "X_prob = softmax(X)\n",
    "print(X_prob, X_prob.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(torch.mm(X.view((-1, num_inputs)), W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数\n",
    "$$Loss = - \\sum\\limits_{j=1}^q y_j^{i}log\\hat y_j^(i)$$\n",
    "$$Cost = - \\frac{1}{n}\\sum\\limits_{j=1}^q y_j^{i}log\\hat y_j^(i) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y = torch.LongTensor([0, 2])\n",
    "y_pred.gather(1, y.view(-1, 1)) # -1 表示不确定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_pred, y):\n",
    "    return - torch.log(y_hat.gather(1, y.view(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义准确率accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y):\n",
    "    return (y_pred.argmax(dim=1) == y).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(y_pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr =5, 0.1\n",
    "\n",
    "def train_softmax(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None,\n",
    "                 lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_pred = net(X)\n",
    "            l = loss(y_pred, y).sum()\n",
    "            \n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            \n",
    "            l.backward()\n",
    "            if optimizer is None:\n",
    "                utils.sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                \n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "             %(epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "train_softmax(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
